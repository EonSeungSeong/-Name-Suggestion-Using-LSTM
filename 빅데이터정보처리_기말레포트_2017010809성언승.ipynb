{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cfc82fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n전반적인 코드는 깃허브 소스코드를 사용하였으며 버전에 따른 에러 및 스파게티코드 위주로 수정을 진행하였고, \\n그래프를 나타내기 위해 코드를 추가하였습니다.\\n이번 레포트의 목표는 LSTM을 이해하고 외국인뿐만아니라 한국인 데이터를 크롤링하여 \\n영문 한국인 이름 데이터를 학습시켰을 때는 어떤 결과가 나오는지 알아봤습니다 . \\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "전반적인 코드는 깃허브 소스코드를 사용하였으며 버전에 따른 에러 및 스파게티코드 위주로 수정을 진행하였고, \n",
    "그래프를 나타내기 위해 코드를 추가하였습니다.\n",
    "이번 레포트의 목표는 LSTM을 이해하고 외국인뿐만아니라 한국인 데이터를 크롤링하여 \n",
    "영문 한국인 이름 데이터를 학습시켰을 때는 어떤 결과가 나오는지 알아봤습니다 . \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7f98a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.데이터셋 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c35eafcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\"\"\"\n",
    "data_name폴더 생성 후 폴더안에 train 및 validation 데이터 셋 저장\n",
    "\"\"\"\n",
    "train_df = pd.read_csv('data_name/train_dataset.csv')\n",
    "val_df = pd.read_csv('data_name/val_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57fba3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #분석에 불필요한 Unknown제거\n",
    "# index=train_df[train_df['origin']=='Unknown'].index\n",
    "# train_df=train_df.drop(index)\n",
    "# index_val=val_df[val_df['origin']=='Unknown'].index\n",
    "# val_df=val_df.drop(index_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8534fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_csv(\"D:\\\\Anaconda\\\\report\\\\data_name\\\\train_dataset2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9284066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33953 3878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "origin\n",
       "African              95\n",
       "American            666\n",
       "Arabic              802\n",
       "Aramaic             131\n",
       "Armenian              2\n",
       "Australian            3\n",
       "Babylonian           13\n",
       "Basque                8\n",
       "British            4205\n",
       "Cambodian             2\n",
       "Celtic              113\n",
       "Chinese              19\n",
       "Czech                34\n",
       "Danish               66\n",
       "Dutch                16\n",
       "Egyptian             13\n",
       "French             1311\n",
       "Gaelic              269\n",
       "German             2354\n",
       "Ghanaian             39\n",
       "Greek              3314\n",
       "Hawaiian            123\n",
       "Hebrew             3188\n",
       "Hungarian            65\n",
       "Indian              262\n",
       "Irish              1133\n",
       "Italian             371\n",
       "Japanese            125\n",
       "Korea               344\n",
       "Latin              3969\n",
       "Modern               75\n",
       "Native American     120\n",
       "Nigerian             12\n",
       "Norse               284\n",
       "Norwegian            18\n",
       "Persian             133\n",
       "Polish               48\n",
       "Portuguese           12\n",
       "Russian             187\n",
       "Sanskrit            150\n",
       "Scandinavian        344\n",
       "Scottish            365\n",
       "Slavic              242\n",
       "Spanish             976\n",
       "Swahili              37\n",
       "Swedish              26\n",
       "Turkish               8\n",
       "Uncertain             5\n",
       "Unknown            7161\n",
       "Vietnamese           10\n",
       "Welsh               635\n",
       "Yiddish              50\n",
       "Name: babyname, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_df), len(val_df))\n",
    "#train_df.tail()\n",
    "#train_df.groupby('sex').babyname.count()\n",
    "train_df.groupby('origin').babyname.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "201934b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c03e3cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 'o', 'n', 's', 'e', 'u', 'n', 'g']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.legacy import data #현재 버전에서 .legacy해줘야 사용가능합니다\n",
    "device = \"cpu\" \n",
    "def tokenizer(text): #데이터 전처리를 위해 알파벳으로 분리하며 리스트로 만들어주는 함수 선언\n",
    "    return list(text.lower())\n",
    "tokenizer(\"EonSeung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10458fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#원핫 함수를 통해 알파벳을 0과 1로 구성된 텐서로 변경\n",
    "def onehot(alist, vocab):\n",
    "    #알파벳에 해당하는 인덱스에 1삽입\n",
    "    _tensor = torch.tensor(alist).data.sub_(1).unsqueeze(1) #.view(-1,1)와 동일\n",
    "    #_tensor = alist.clone().detach().data.sub_(1).unsqueeze(1)\n",
    "    \n",
    "    #행: 이름을 구성하는 알파벳 수 \n",
    "    #열: 이름을 구성하는 문자열 수: 알파벳+ eos,bos등 -->35개\n",
    "    _onehot = torch.zeros((len(alist), len(vocab) - 1), dtype=torch.float)\n",
    "    \n",
    "    _onehot.scatter_(1, _tensor, 1)\n",
    "    return _onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f01a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "\n",
    "    def __init__(self, data_dir, batch_size):\n",
    "        #\n",
    "        self.BABYNAME = data.Field(sequential=True, tokenize=tokenizer,\n",
    "                                   batch_first=True, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "        \n",
    "        self.SEX = data.Field(sequential=False, use_vocab=True, postprocessing=onehot)\n",
    "        self.ORIGIN = data.Field(sequential=False, use_vocab=True, postprocessing=onehot)\n",
    "\n",
    "        self.train_ds, self.val_ds = data.TabularDataset.splits(\n",
    "            #수정\n",
    "            path=data_dir, skip_header=True, train='train_dataset.csv',\n",
    "            validation='val_dataset.csv', format='csv',\n",
    "            fields=[('babyname', self.BABYNAME), ('sex', self.SEX), ('origin', self.ORIGIN)]\n",
    "        )\n",
    "        #글자종류 count하기 위해 함수선언\n",
    "        self.build_vocab()\n",
    "\n",
    "        self.train_iter, self.val_iter = data.BucketIterator.splits(\n",
    "            (self.train_ds, self.val_ds), batch_sizes=(batch_size, batch_size), device=device,\n",
    "            repeat=False, sort_key=lambda x: len(x.babyname))\n",
    "\n",
    "    def build_vocab(self):\n",
    "        self.BABYNAME.build_vocab(self.train_ds, self.val_ds)\n",
    "        self.SEX.build_vocab(self.train_ds, self.val_ds)\n",
    "        self.ORIGIN.build_vocab(self.train_ds, self.val_ds)\n",
    "        print(\"정상적으로 실행되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "923ecb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정상적으로 실행되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#data_dir ='data_name/full_version'\n",
    "data_dir ='data_name'\n",
    "batch_size = 16\n",
    "data_loader = DataLoader(data_dir, batch_size)\n",
    "sample = next(iter(data_loader.train_iter))\n",
    "sex = sample.sex.float()\n",
    "origin = sample.origin.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d6fb1f3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2])\n",
      "torch.Size([16, 52])\n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.legacy.vocab.Vocab object at 0x00000209F2166370>>, {'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3, 'a': 4, 'e': 5, 'n': 6, 'i': 7, 'l': 8, 'r': 9, 'o': 10, 's': 11, 't': 12, 'h': 13, 'd': 14, 'y': 15, 'm': 16, 'c': 17, 'u': 18, 'k': 19, 'b': 20, 'g': 21, 'j': 22, 'v': 23, 'p': 24, 'f': 25, 'w': 26, 'z': 27, 'q': 28, 'x': 29, ' ': 30, '-': 31, \"'\": 32, '.': 33, ',': 34})\n"
     ]
    }
   ],
   "source": [
    "print(sex.size())\n",
    "print(origin.size())\n",
    "print(data_loader.BABYNAME.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c50bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input target data 선언\n",
    "inputs = sample.babyname[:, :-1] #처음부터 마지막 직전까지\n",
    "targets = sample.babyname[:, 1:] #둘째부터 마지막까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea165401",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  5, 12, 13,  8,  7,  6, 14,  4,  3],\n",
       "        [ 2, 20,  9,  4,  6, 17,  4,  3,  1,  1],\n",
       "        [ 2,  5, 14,  9,  7, 17, 19,  3,  1,  1],\n",
       "        [ 2, 11, 12,  5, 24, 13,  4,  6,  5, 15],\n",
       "        [ 2, 26, 15,  8,  4,  6,  3,  1,  1,  1],\n",
       "        [ 2, 16,  4, 17, 21, 18,  7,  9,  5,  3],\n",
       "        [ 2, 19, 18, 16,  4,  9,  3,  1,  1,  1],\n",
       "        [ 2, 17,  4, 12, 13,  9,  7,  6,  6,  3],\n",
       "        [ 2, 12,  7, 25, 25,  7,  6, 15,  3,  1],\n",
       "        [ 2,  4, 23, 14, 10, 12, 15,  4,  3,  1],\n",
       "        [ 2, 23, 10,  6,  6,  5, 11, 11,  4,  3],\n",
       "        [ 2, 17,  4,  7, 11,  7, 14,  5,  3,  1],\n",
       "        [ 2, 12,  4, 16,  4, 11,  7,  6,  5,  3],\n",
       "        [ 2, 10, 16,  5,  9, 10,  3,  1,  1,  1],\n",
       "        [ 2, 19,  7,  9, 11, 12,  7,  6,  3,  1],\n",
       "        [ 2,  4,  8, 16,  5, 14,  4, 13,  3,  1]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#각행의 첫번째 인덱스 -> 2로시작 2(<bos>)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b4e8833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 12, 13,  8,  7,  6, 14,  4,  3,  1],\n",
       "        [20,  9,  4,  6, 17,  4,  3,  1,  1,  1],\n",
       "        [ 5, 14,  9,  7, 17, 19,  3,  1,  1,  1],\n",
       "        [11, 12,  5, 24, 13,  4,  6,  5, 15,  3],\n",
       "        [26, 15,  8,  4,  6,  3,  1,  1,  1,  1],\n",
       "        [16,  4, 17, 21, 18,  7,  9,  5,  3,  1],\n",
       "        [19, 18, 16,  4,  9,  3,  1,  1,  1,  1],\n",
       "        [17,  4, 12, 13,  9,  7,  6,  6,  3,  1],\n",
       "        [12,  7, 25, 25,  7,  6, 15,  3,  1,  1],\n",
       "        [ 4, 23, 14, 10, 12, 15,  4,  3,  1,  1],\n",
       "        [23, 10,  6,  6,  5, 11, 11,  4,  3,  1],\n",
       "        [17,  4,  7, 11,  7, 14,  5,  3,  1,  1],\n",
       "        [12,  4, 16,  4, 11,  7,  6,  5,  3,  1],\n",
       "        [10, 16,  5,  9, 10,  3,  1,  1,  1,  1],\n",
       "        [19,  7,  9, 11, 12,  7,  6,  3,  1,  1],\n",
       "        [ 4,  8, 16,  5, 14,  4, 13,  3,  1,  1]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#각행의 마지막 인덱스 -> 1또는 3으로시작 1(<pad>) 3(<eos>)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0608e056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_input: torch.Size([16])\n",
      "embed_output torch.Size([16, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "#num_embeddings=35 이름을 구성하는 글자의 수\n",
    "#embedding_dim=100 임베딩 차원-> 100차원\n",
    "embedding_layer = nn.Embedding(num_embeddings=35, embedding_dim=100)\n",
    "\n",
    "# input 에서 첫 글자를 선택해 레이어에 집어넣는다.\n",
    "sample_input = inputs[:, 0]\n",
    "print(\"sample_input:\", sample_input.size())\n",
    "\n",
    "embed_output = embedding_layer(inputs[:, 0])\n",
    "\n",
    "#첫 글자를 100차원의 어떤 숫자로 표현한다.\n",
    "print(\"embed_output\", embed_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14de5e50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_output:  torch.Size([16, 100])\n",
      "origin:  torch.Size([16, 52])\n",
      "sex:  torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"embed_output: \", embed_output.size())#이름\n",
    "print(\"origin: \", origin.size())#국가\n",
    "print(\"sex: \", sex.size())#성별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea5bcece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_combined: torch.Size([16, 154])\n"
     ]
    }
   ],
   "source": [
    "inputs_combined = torch.cat((embed_output, origin, sex), dim=1)\n",
    "print(\"inputs_combined:\", inputs_combined.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7cb2cb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1590,  0.9671, -0.7479,  1.0814,  0.4326], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_combined[0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52ad45bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 154])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_combined.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f34952ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69fac565",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=154, hidden_size=200, batch_first=True) \n",
    "#batch_first=True-> 데이터 차원 혼동 방지위해 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c79300f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hx = torch.zeros(1, 16, 200) #history -> 정보누적됨\n",
    "cx = torch.zeros(1, 16, 200) #Cell state 텐서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc427445",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out, (hx, cx) = lstm.forward(inputs_combined.unsqueeze(1), (hx, cx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "54d2f7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 200])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out.size() #batch_first=True로 인하여 batch값이 먼저나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3878f3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm.weight_ih_l0.size(), lstm.weight_hh_l0.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0e5917d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_inter = nn.Linear(200, 300)\n",
    "#오버피팅 방지\n",
    "drop = nn.Dropout(0.5)\n",
    "decoder = nn.Linear(300, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e8d7dd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 300])\n"
     ]
    }
   ],
   "source": [
    "fc_inter_out = fc_inter.forward(lstm_out.squeeze(1))\n",
    "print(fc_inter_out.size())\n",
    "drop_out = drop.forward(fc_inter_out)\n",
    "decoder_out = decoder.forward(drop_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f98e70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 35])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "86959e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0509, -0.0119,  0.0335,  0.0835,  0.0072,  0.0114, -0.0737, -0.0974,\n",
       "         0.0270,  0.0186,  0.0286,  0.0364, -0.0868, -0.0363,  0.0505, -0.0110,\n",
       "         0.0173, -0.0176,  0.0388, -0.0367, -0.0541,  0.0585,  0.0544, -0.0109,\n",
       "         0.0674,  0.0140, -0.0497,  0.1043, -0.0171, -0.0558,  0.0560,  0.0424,\n",
       "         0.0762,  0.1241, -0.0073], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_out[0] #구성된 35개 글자로 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58d8d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.nn.functional as F\n",
    "# softmax_out = F.softmax(decoder_out, dim=1)\n",
    "# softmax_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4c63d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax_out[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eba4985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim,\n",
    "                 nb_sex, nb_origin,\n",
    "                 lstm_nb_layers, lstm_hidden_dim,\n",
    "                fc_out, dropout_p):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.nb_sex = nb_sex\n",
    "        self.nb_origin = nb_origin\n",
    "        self.lstm_nb_layers = lstm_nb_layers\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.fc_out = fc_out\n",
    "        self.dropout_p = dropout_p\n",
    "        self.device =\"cpu\"\n",
    "        #이전 선언했던 임베딩, 모델 선언\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim + nb_sex + nb_origin,\n",
    "                            lstm_hidden_dim, num_layers=lstm_nb_layers,\n",
    "                            batch_first=True)\n",
    "        self.fc_inter = nn.Linear(lstm_hidden_dim, fc_out)\n",
    "        #오버피팅 방지를 위해 드랍아웃 선언\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.decoder = nn.Linear(fc_out, vocab_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(\n",
    "                    self.lstm_nb_layers,\n",
    "                    batch_size,\n",
    "                    self.lstm_hidden_dim\n",
    "                ).float().to(self.device),\n",
    "                torch.zeros(\n",
    "                    self.lstm_nb_layers,\n",
    "                    batch_size,\n",
    "                    self.lstm_hidden_dim\n",
    "                ).float().to(self.device))\n",
    "\n",
    "    def forward(self, origin, sex, inputs, hidden, isDebug=False):\n",
    "        if isDebug: print(\"origin:\", origin.size())\n",
    "        if isDebug: print(\"sex:\", sex.size())\n",
    "        if isDebug: print(\"inputs:\", inputs.size())\n",
    "\n",
    "        embed = self.embedding(inputs)\n",
    "        if isDebug: print(\"embed:\", embed.size())\n",
    "\n",
    "        inputs_combined = torch.cat([origin, sex, embed], dim=1)\n",
    "        if isDebug: print(\"inputs_combined:\", inputs_combined.size())\n",
    "\n",
    "        lstm_out, hidden = self.lstm(inputs_combined.unsqueeze(1), hidden)\n",
    "        if isDebug: print(\"lstm_out:\", lstm_out.size())\n",
    "        if isDebug: print(\"last_hidden_state:\", hidden[0].size())\n",
    "        if isDebug: print(\"last_cell_state:\", hidden[1].size())\n",
    "\n",
    "        fc_inter_out = self.fc_inter(lstm_out.squeeze(1))\n",
    "        if isDebug: print(\"fc_inter_out:\", fc_inter_out.size())\n",
    "\n",
    "        dropout_out = self.dropout(fc_inter_out)\n",
    "\n",
    "        decoder_out = self.decoder(dropout_out)\n",
    "        if isDebug: print(\"decoder_out:\", decoder_out.size())\n",
    "\n",
    "        return decoder_out, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8ddd73ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Net(\n",
      "  (embedding): Embedding(35, 100)\n",
      "  (lstm): LSTM(154, 200, batch_first=True)\n",
      "  (fc_inter): Linear(in_features=200, out_features=300, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (decoder): Linear(in_features=300, out_features=35, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "#모델 변수 선언\n",
    "model = Net(vocab_size=35, embedding_dim=100,\n",
    "                 nb_sex=2, nb_origin=52,\n",
    "                 lstm_nb_layers=1, lstm_hidden_dim=200,\n",
    "                fc_out=300, dropout_p=0.5, \n",
    "           ).to(device)\n",
    "print(model.parameters)\n",
    "sample = next(iter(data_loader.train_iter))\n",
    "\n",
    "sample_input = sample.babyname[:, :-1].to(device)\n",
    "sample_target = sample.babyname[:, 1:].to(device)\n",
    "sample_origin = sample.origin.float().to(device)\n",
    "sample_sex = sample.sex.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "252405a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hx, cx 배치값으로 초기화\n",
    "(hx, cx) = model.init_hidden(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4390847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cx의 첫번쨰 값을 모델에 적용하여 그래프로 출력\n",
    "cx_tmp = [cx[0, 0, 0].item()]\n",
    "for str_index in range(sample_input.size(1)):\n",
    "    output, (hx, cx) = model(sample_origin, sample_sex, sample_input[:, str_index], (hx, cx))\n",
    "    cx_tmp.append(cx[0, 0, 0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5a2fb332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.11273039132356644, 0.13863664865493774, 0.12640617787837982, -0.15233594179153442, -0.32454702258110046, -0.11768341809511185, -0.08328302949666977, 0.19084608554840088, 0.18419045209884644, -0.21852068603038788, -0.4730810523033142]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAqOElEQVR4nO3deVxV953/8deHHVQ2QYQLghrEGDfwxpo9aTTVxGhippmkbZJpmzjdZtpOt3Tm95hp5/fr45fO9NdlpkvG2LRpm2naabQxxsSY3ZjVFde4oqwCIoKyw+f3B5cGFRW493LuPffzfDx43IXD/X6uy5vDh3M+R1QVY4wx7hfldAHGGGNGhgW+McZECAt8Y4yJEBb4xhgTISzwjTEmQsQ4XcDFZGRkaEFBgdNlGGNM2NiyZUu9qmYO9LmQDvyCggI2b97sdBnGGBM2ROTohT5nLR1jjIkQFvjGGBMhAhL4IrJQRD4QkYMi8vAAn/+kiJT6Pt4SkVmBWNcYY8zg+R34IhIN/AxYBEwD7hWRaedsdgS4QVVnAv8bWOHvusYYY4YmEHv4c4GDqnpYVTuAp4Cl/TdQ1bdU9aTv4TtAbgDWNcYYMwSBCHwPUN7vcYXvuQv5LPB8ANY1xhgzBIE4LFMGeG7AEZwichO9gX/tBV9MZDmwHGDChAkBKM8YYwwEJvArgLx+j3OBqnM3EpGZwEpgkaqeuNCLqeoKfD1+r9drs5uNCQP7jzfz4u4aUhJjSUmKIy0pltTEOFKTYklNimV0fAwiA+0bmpEUiMB/HygUkYlAJXAP8In+G4jIBGAVcJ+q7g/AmsaYEPLdZ3ez6eAF9+OIjhJSE2N93wDifPd93xASY0kdFfeXz6clxZGSaN8ogsHvwFfVLhH5ErAeiAYeV9XdIvI53+cfBf4ZGAv83PeX16WqXn/XNsY4r665nbcPneDzN07mM9dM5FRrBydbOmls6aSxpaP3ttV367tf09TGvppmGls6ONPRfcHXjokSUpNiSUns/UbQe7/3J4h75uZx2bgxI/hOw19ARiuo6jpg3TnPPdrv/oPAg4FYyxgTWl7YVU2Pwh2zPWSOiSdzTPyQvr6jq4fG1g5OtXT6vlF00Nja/5vFh/erGtvYU9XE8eZ2apvb+Y97i4P0rtwppGfpGGNC39rSai4bN5opWaOH9fVxMVGMG5PAuDEJg/6aL/73VrYcPXnpDc1ZbLSCMWbYjje18V5ZA4tnZo9or92bn0ZlYytVja0jtqYbWOAbY4Zt3c5qVGHxzJwRXdebnw7AZtvLHxILfGPMsK0trWbq+DFcNm547Zzhujx7DElx0WwpaxjRdcOdBb4xZliqGlvZcvQkt88a2b17gJjoKGbnpdoe/hBZ4BtjhmXdzmoAbpuR7cj63vw09lY3cbq9y5H1w5EFvjFmWJ4trWa6J5mCjFGOrD+nIJ0ehe3HGh1ZPxxZ4Btjhqy8oYUd5Y0j/sva/oonpCICm49aH3+wLPCNMUO2ttTZdg5AckIsRVlj7Hj8IbDAN8YM2XM7q5iVl0peepKjdXgL0th2rJHuHpuzOBgW+MaYISmrP8OuyiZun+nc3n0fb346p9u72FfT5HQpYcEC3xgzJGtLe6ef3+pgO6fPnPw0AGvrDJLN0gkD7V3dbC47yRv763h9fx21ze3kpiWSl57EhPQk8tJ6byekJ5GdmkBstH0fN8GztrQab34aOamJTpdCbloiWcnxbC47yf1XFThdTsizwA9BqkrZiRZe/6CWNw7U8/ahE7R2dhMbLXjz0ymekErFyVZ2V57ixd01dHZ/2L+MjhKyUxL+8g0gz/fR9zgtKdbmi5thO1jbzL6aZr5z+zSnSwFARPAWpNse/iBZ4IeI0+1dvHWwntf31/HGgTrKG3qHQhWMTeLj3lxumJLJvEljGRV/9l9Zd49S09TGsRMtlJ9sobyhhWO+j5f21lJ/uv2s7UfFRZ/1DSCv321uWiIJsdEj9p5N+FlbWo0ILAqBdk4fb34az5VWU9XYGhI/dYQyC3yH9PQoe6qbegN+fx1bjp6kq0dJiovm6skZLL9uEtdPySR/7MVPaomOEjypiXhSE7mKsed9vqWji/KGVo41fPjNoLyhhbITZ3jjQB1tnT1nbZ+VHP/hTwa+VtH8y7NISYoN6Ps34UdVWVtazdyCdLKSBz/KONj6D1JbYoF/URb4I6j+dDsbD9Txxv56Nh6oo/50BwDTspN56PpJXF+YyZz8NOJiAteDT4qLoWj8GIrGn39lIFWl7nQ75Q0tf/mm0PcN4Z1DJ1jdVIkq/LU3j+//1cyA1WTC0wfHmzlYe5oH7pjudCln6T9IbYkDc33CiQV+EHV297D16Mm/tGl2VfYeOpY+Ko7rCjO4vjCT66ZkDOnCD4EkIn+58MSc/PM/397Vzdf+uIMXdtfwf+6cbr8MjnBrd1QTJbBo+ninSzmLDVIbPAv8ACtvaOF139E0bx86wen2LqKjhDkT0vj6LVO4fkom03NSiIoK/V+cxsdEc8dsD2tLq9l0sJ4bi8Y5XZJxSG87p4qrJ2eQMXpolzAcCd78NH766kFOt3cxOt5i7ULsT8ZPqsobB+p5dV8tr++v40j9GQA8qYksmZ3D9YWZXH3ZWJITwrMHft2UDMbEx7BuZ7UFfgTbXdVE2YkW/vaGyU6XMqD+g9SuLcxwupyQZYHvpx9u2M9/vnKQhNgo5k0ay33z8rmhKJNJGaNccfhjfEw086dlsX73cb53Z4+1dSLU2tJqYqKEhVeEVjunT/9Bahb4FxaQ/70islBEPhCRgyLy8ACfnyoib4tIu4h8PRBrhoI1O6r4z1cO8vE5uWz/51v49afn8plrJzI5c7Qrwr7PbTOyOdXayaaD9U6XYhzQ18655rIM0kbFOV3OgGyQ2uD4HfgiEg38DFgETAPuFZFzz8poAP4e+IG/64WK0opGvvE/O7iyII3v3TnD1cev92/rmMizo+IUFSdbWRwCs3MuxgapXVog9vDnAgdV9bCqdgBPAUv7b6Cqtar6PtAZgPUcV9vUxvLfbCFjdDy/+NScgB5GGYr6t3U6u3su/QXGVZ4rrSI2WrglRNs5fWyQ2qUFIqk8QHm/xxW+54ZFRJaLyGYR2VxXV+d3cYHW1tnNQ7/dQlNbJ4/d7w3JIxaCwdo6kamnR3mutJrrCzNJSQztAw9skNqlBSLwB2pWD/tnKlVdoapeVfVmZmb6UVbgqSrfXrWTHeWN/PDu2UzLSXa6pBFjbZ3ItK38JFWn2lg8K7TbOdA7SG18cgKbyyzwLyQQgV8B5PV7nAtUBeB1Q85/vXGY1dsq+YcFU1gYYiefBJu1dSLTszuqiYuJYv7lWU6XckkiwpyCNNvDv4hABP77QKGITBSROOAeYE0AXjekvLz3ON9/YR+3zczm7z56mdPlOMLaOpGlu0dZt7Oam4oyGRMm55F489OobGylqrHV6VJCkt+Br6pdwJeA9cBe4I+qultEPicinwMQkfEiUgH8A/C/RKRCRMKmH7L/eDNffmo7V+Qk84O/muWqQy6Hwto6kWVzWQO1ze3c5uCFyoeq/yA1c76AHF6iqutUdYqqTlbV7/mee1RVH/Xdr1HVXFVNVtVU3/2w+FX6yTMdPPjEZhJio1lxn5fEOPcefnkp1taJLGtLq0mIjeLmqeFzhnX/QWrmfO4+ntBPnd09fOHJrdScamPF/XNs1jbW1okUXd09PL+rmpunZp13DYZQZoPULs4C/yL+9dk9vH34BP932QxKJqQ5XU5IsLZOZHj3SAP1pztC/mSrgXjz09hb3cTp9i6nSwk5FvgX8Lt3jvLbd46y/PpJ3DUn1+lyQoa1dSLD2tIqkuKiuSmM2jl9+g9SM2ezwB/A24dO8J01u7mpKJNvLZzqdDkhx9o67tbZ3cMLu2qYf3lWWI4M6T9IzZzNAv8cx0608Pknt1CQMYqf3FtMdBjMrR9p1tZxt7cOneBkS2dYtnPABqldjAV+P81tnTz4m/dRhZX3e8N2hn2wWVvH3dbuqGJMfAw3FIXWme5DYYPUBmaB79Pdo3z1D9s5VHeGn3+yhIKMi188PNJZW8edOrp6WL+7hgVXZBEfE37tnD42SG1gFvg+P3jxA17aW8s/L57GNZfZBRQupa+t81yptXXcZOOBOpraurg9jE62Goi3wAapDcQCH/jztkp+8doh7p07gfuvGuBq3uY88THRLJiWxYt7rK3jJs+VVpOSGBv2Oz2eVBukNpCID/zt5Y188+lS5k5M57tLrojYsQnDcau1dVylrbObF/cc52NXZIX9NR5skNrAwvtv1U81p9pY/pvNjBsTz6MRcCGTQLO2jru8vr+O0+1dLA7zdk4fG6R2vohNuLbObpb/djNn2rtY+YCX9BC9Vmcos7aOu6wtrSYtKZarJ491upSAsEFq54vIwFdVvvmnUnZWnuLH9xQzdXzYDO4MOdbWcYfWjm5e3nuchdOziYl2RyzYILXzueNvdoh+/toh1uyo4uu3FLFgWuhf2CGUWVvHHV79oJaWjm5uD9OTrQZig9TOF3GBv2HPcX7w4gcsnZ3DF26c7HQ5Yc/aOu6wtrSKjNHxfGSSO9o5fWyQ2tkiKvD31TTxlae2MdOTwvfvmmlH5ASItXXC25n2Ll7ZV8utM8a7bpSIDVI7W8QEfoPvQiaj4mP4r/u8YTkUKlRZWye8vbT3OG2dPa45Oqc/G6R2togI/I6uHj7/uy3UNrez4n4v41MSnC7JVaytE96eK60mKzkeb777rvlgg9TO5vrAV1W+8+xu3j3SwL//1Uxm56U6XZIrWVsnPDW3dfLa/jpunZFNlMvaOX1skNqHXB/4v33nKP/97jE+f+Nkls72OF2Oa1lbJzxt2HOcji53tnP6XFlgg9T6uDrwNx2s57vP7mH+5eP4xi1FTpfjatbWCU9rS6vxpCZSMiHV6VKCZk6+DVLrE5DAF5GFIvKBiBwUkYcH+LyIyH/4Pl8qIiWBWPdiyurP8IUntzI5cxQ/vqfYtT+uhhJr64SXUy2dbDxQx20zs119xJoNUvuQ34EvItHAz4BFwDTgXhGZds5mi4BC38dy4Bf+rnsxTW2dPPibzUQJrLz/SkbHxwRzOeNjbZ3wsn5PDZ3dym0z3HOy1UBskNqHArGHPxc4qKqHVbUDeApYes42S4HfaK93gFQRCcq/su4e5cu/30ZZ/Rl+/sk5TBibFIxlzACsrRNe1pZWMyE9iZm5KU6XEnQ2SK1XIALfA5T3e1zhe26o2wAgIstFZLOIbK6rqxtyMafbuzjV2sl3llzBVS4ZAhVOrK0THhrOdLDpYL3r2zl9bJBar0AE/kD/Ws49/mkw2/Q+qbpCVb2q6s3MHPo1NVMSY/nj317Fp+bZhUycYG2d8PDCrhq6ezRsL1Q+VDZIrVcgAr8CyOv3OBeoGsY2AeOWaX/hyNo64WFtaRWTMkYxLTsyJsXaILVegUjG94FCEZkoInHAPcCac7ZZA9zvO1pnHnBKVW0X0KWsrRPa6prbeefwiYhp5/SxQWoBCHxV7QK+BKwH9gJ/VNXdIvI5Efmcb7N1wGHgIPAY8AV/1zWhy9o6oe2FXdX0KK4+2WogNkgNAnK8oqquozfU+z/3aL/7CnwxEGuZ0Ne/rfO9rh67dGSIeba0msJxoykaP8bpUkZU/0Fq1xaG90Xah8v+J5qg+Etb55C1dULJ8aY23i9riLi9e7BBamCBb4Kkr62zzto6IWXdzmpU4bYIOTrnXFcWpEf0IDULfBMU/ds6HV12tE6oWFtazdTxY7hs3GinS3GEtyAtogepWeCboLG2Tmipamxly9GT3D4r8to5fSJ9kJoFvgkaa+uElr6jpiLlZKuBRPogNQt8EzTW1gkta0urmOFJIX/sKKdLcUzfILXNEXrGrQW+CSpr64SG8oYWdlScithf1vbnzU+j6lRbRA5Ss8A3QWVtndCw1vfn7/ZRyIMRyYPULPBNUFlbJzSsLa1idl4qeek2LjySB6lZ4Jugs7aOs47Un2F3VVNE/7K2v0gepGaBb4LO2jrOWrujdzCt9e8/FKmD1CzwTdBZW8dZz+2sxpufRnZKotOlhIxIHaRmgW9GhLV1nHGwtpl9Nc3WzjlH/0FqkcQC34wIa+s449kd1Yj0fsM1H4rUQWoW+GZEWFtn5Kkqa0ur+MjEdMYlJzhdTsiJxEFqFvhmxFhbZ2Ttq2nmUN2ZiByFPBiROEjNAt+MGGvrjKznSquJElg4fbzTpYSkSBykZoFvRoy1dUZOXzvn6skZZIyOd7qckBSJg9Qs8M2IsrZOcKkq28sb+cfVuyg70WJH51xEJA5SC8g1bY0ZrP5tnZuKxjldjmtUn2pl1dZKVm2t4FDdGeJjolhW4mHpbI/TpYU0b34az5VWU9XYSk6q+89TsMA3I6qvrbN+dw3fu3OGXeDcDy0dXazfXcPTWyrZdKgeVZhbkM5D103i1pnZJCfEOl1iyOs/SG2JBf7FiUg68AegACgD7lbV8xpiIvI4sBioVdXp/qxpwt9tM7NZta03pGwvf2h6epR3jzTw9NYKnt9ZzZmObvLSE/n7jxayrMQT0bPuh6P/ILUlEXAlMH/38B8GXlbVR0TkYd/jbw2w3a+BnwK/8XM94wLXFlpbZ6jK6s+wamsFq7ZVUnGyldHxMSyemcOyEg9XFqQTFSVOlxiWIm2Qmr+BvxS40Xf/CeA1Bgh8VX1DRAr8XMu4hLV1BudUayfPlVbz9NYKthw9SZTAtYWZfONjRdwybTyJcdFOl+gK3vw0fvrqQU63dzE63t1dbn/fXZaqVgOoarWI+L27JiLLgeUAEyZM8PflTIiyts7Aurp72Hignj9trWCD7/DVwnGjeXjRVO6Y7WF8ip0xG2j9B6ldW5jhdDlBdcnAF5GXgIHO3PinwJcDqroCWAHg9Xoj55znCGNtnbPtrW5i1dYK/ry9irrmdtKSYvnE3AksK/Eww5OCiLVsgqX/ILWID3xVnX+hz4nIcRHJ9u3dZwO1Aa3OuJa1daD+dDvPbK/i6S0V7KluIjZauKloHHfNyeWmonER+WfihOSEWKaOT46IM279bemsAR4AHvHdPuN3RSZiRGJbp72rm5f31rJqawWvfVBHV48yMzeF7y65gttn5ZA+Ks7pEiOSNz+N1dsq6e5Rol38C3B/A/8R4I8i8lngGPBxABHJAVaq6q2+x7+n95e7GSJSAfyLqv7Sz7VNmIu0ts5PXjrA45uOcKq1k6zkeB68bhJ3lXgozBrjdGkRz1uQxm/fOcq+miauyElxupyg8SvwVfUEcPMAz1cBt/Z7fK8/6xh3iqS2zq7KU/zopf3cWJTJZ66ZyDWXZbh6TzLc9B+k5ubAd+//MBMWbpuZTVNbl+tn6zy28TCj42P4j3uLuX5KpoV9iOkbpPa+ywepWeAbR/Vv67hVVWMra0uruefKPBt3EKL6BqltcfkgNQt846j+bR23jkz+9VtlAHz62onOFmIuypufRtWpNqoaW50uJWgs8I3j3NzWaW7r5PfvHuPWGdl4ImA4VzjrP0jNrSzwjePc3Nb5w/vlNLd38dB1tncf6voPUnMrC3zjOLe2dbq6e/jVpjLmTkxnZm6q0+WYS4iEQWoW+CYkuLGt8/yuGiobW3nouklOl2IGyZufxt7qJk63dzldSlBY4JuQ4La2jqqycuNhJmWM4uap7j+pzC36D1JzIwt8ExLc1tZ5v+wkOypO8ZlrJ9qs+jBSMiGVKN8gNTeywDch4/ZZOTS1dfHsjiqnS/HbYxsPk5YUy10luU6XYoZgTEIsRS4epGaBb0LGDVMymeFJ4Ycb9tPW2e10OcN2pP4ML+09zn3z8u0iJWHIm5/GtmONdPe4bzq7Bb4JGVFRwsOLplLZ2Mrv3jnqdDnD9ss3DxMbHcV9VxU4XYoZBm9BGqfbu9hX0+R0KQFngW9CyjWXZXD9lEx++upBmto6nS5nyE6e6eBPWyq4c7aHzDHxTpdjhqH/IDW3scA3IedbC4tobOnk0dcOOV3KkP3unaO0dfbwoJ1oFbbcPEjNAt+EnCtyUrhjdg6PbzpCzak2p8sZtLbObp54+yg3FmXajPsw5uZBahb4JiR97ZYiunuUn7y83+lSBm3N9irqT7fbiVYu4NZBahb4JiTlpSfxqXn5/OH9cg7Wnna6nEtSVVa+eZjLs5O5evJYp8sxfnLrIDULfBOyvnTTZSTFxfDv6/c5Xcolvb6/jv3HT/PQdRMRsROtwp1bB6lZ4JuQNXZ0PH97/STW7z4e8kdMrNx4hKzkeBbPzHG6FBMAbh2kZoFvQtpnr5tI5ph4vv/8PlRD80SYPVVNvHmwnr+5eqKrr8sbadw4SM2vf50iki4iG0TkgO82bYBt8kTkVRHZKyK7ReTL/qxpIktSXAxfmV/Ie2UNvLy31ulyBrTyzcMkxUXzibkTnC7FBJDXhYPU/N0deRh4WVULgZd9j8/VBXxNVS8H5gFfFJFpfq5rIsjd3jwmZYzi+y/sC7nT3Y83tfHsjiru9uaRkmTXq3WTYhcOUvM38JcCT/juPwHcce4Gqlqtqlt995uBvYDHz3VNBImNjuIbHyviQO1pnt5a4XQ5Z/n1W2V09yiftevVuo4bB6n5G/hZqloNvcEOXHTwt4gUAMXAu36uayLMwunjmZWXyo9CaLDamfYunnznKAunjycvPcnpckwQuG2Q2iUDX0ReEpFdA3wsHcpCIjIaeBr4iqpecCqRiCwXkc0isrmurm4oSxgXExG+vWgq1afaeOKtMqfLAeB/NpfT1NbFg3ailWu5bZDaJQNfVeer6vQBPp4BjotINoDvdsDfqolILL1h/6SqrrrEeitU1auq3szMzKG/I+Na8yaN5aaiTH726kFOtTg7WK27R3l8Uxlz8tMomXDesQrGJdw2SM3fls4a4AHf/QeAZ87dQHrPQvklsFdVf+jneibCfWvRVJrbu/j5awcdrePF3TUca2jhIRuS5mpuG6Tmb+A/AiwQkQPAAt9jRCRHRNb5trkGuA/4qIhs933c6ue6JkJNHZ/MsuJcfvVWmaNzTh7beJj8sUksmDbesRpM8LltkJpfga+qJ1T1ZlUt9N02+J6vUtVbffffVFVR1ZmqOtv3se7ir2zMhf3DLVMA+NEGZwarbTl6kq3HGvnMNROJtuvVup6bBqnZaYEm7HhSE3ngqnye3lrBBzXNI77+yo2HSUmM5eNeu15tJLh6cgYAG/Ycd7gS/1ngm7D0hRsvY1T8yA9WO3aihfW7a/jkRyaQFBczomsbZxSNH8Pl2cms2lbpdCl+s8A3YSltVByfv3EyL+2t5b0jI9dffXzTEaKjhAeuLhixNY3zlhV72FHeyKG60B/VfTEW+CZsffrqiYxPTuCR5/eOyGC1Uy2d/HFzOUtmechKTgj6eiZ0LJ2dQ5TA6q3hvZdvgW/CVmJcNF9dUMjWY42s3x38/uqT7x2lpaPbrlcbgcYlJ3BtYSart1XSE8Zn3Vrgm7B2V0kul40bzb+t30dXd0/Q1uno6uGJt8q4rjCDy7OTg7aOCV13lXiobGzlvTA+RNMC34S1mOgovvmxIg7XneF/tgRvsNqzO6o43tRuYxQi2C3TxjMqLppVITbAbygs8E3YWzAtizn5afxow35aOwI/WE1VeWzjYYqyxnB9YUbAX9+Eh8S4aBZOz2bdzpqQGeA3VBb4JuyJCA8vmkptczuPbzoS8NffdPAE+2qa+axdrzbi3VXi4XR7Fy+G6TH5FvjGFa4sSGf+5Vk8+tohTp7pCOhrP7bxMJlj4lk6265XG+nmTRpLdkoCq8O0rWOBb1zjWwuLONPRxU9fDdxgtf3Hm3l9fx0PXJVPfEx0wF7XhKeoKOGOYg9vHKinrrnd6XKGzALfuEZh1hg+PieP3759lPKGloC85sqNh0mIjeKTH8kPyOuZ8Les2EN3j7JmR5XTpQyZBb5xla8sKEQkMIPVapvb+PO2Kj4+J4+0UXEBqM64QWHWGGZ4Uli9LfzaOhb4xlWyUxL59DUTWb29kj1V/l2l6LdvH6Wzp8euV2vOc2exh12VTew/PvLD+/xhgW9c5/M3TCY5IZZ/82OwWmtHN7975ygLLs+iIGNUAKszbrBkdg7RUcKqMBu1YIFvXCclKZYv3jSZ1z6o461D9cN6jT9treBkSycPXW8nWpnzZYyO54Ypmfx5W2VYXeDcAt+40v1XFZCTksD3n9835MFqPT3K428eYVZeKt58u16tGdiyEg81TW28c/iE06UMmgW+caWE2Gi+umAKOypOsW5nzZC+9qW9xzlSf4aH7EQrcxHzL89iTHwMT4fRMfkW+Ma1lpXkUpQ1hn9fv4/OIQxWW7nxCJ7URBZeYderNReWEBvNbTOzeWFXDS0dXU6XMygW+Ma1oqOEby0qouxEC0+9Xz6or9lR3sh7ZQ185tqJxETbfw9zcXcWe2jp6Gb97qH9FOkU+xdtXO2monHMnZjOT146wJn2S++FPbbxMGMSYvjrK/NGoDoT7q4sSCc3LTFsjtbxK/BFJF1ENojIAd/teb/hEpEEEXlPRHaIyG4R+a4/axozFH2D1epPt/PLNy8+WK3iZAvP76rhE3MnMDrerldrLi0qSriz2MOmg/Ucb2pzupxL8ncP/2HgZVUtBF72PT5XO/BRVZ0FzAYWisg8P9c1ZtBKJqSx8Irx/Nfrhzhx+sLzT361qQwB/uaaghGrzYS/O4s99Cg8sz309/L9DfylwBO++08Ad5y7gfbqu/JvrO8jfA5cNa7wjYVFtHX18J+vDDxYramtkz+8X87imdlkpySOcHUmnE3KHM3svNSwaOv4G/hZqloN4LsdN9BGIhItItuBWmCDqr57oRcUkeUisllENtfV1flZnjG9JmeO5m5vHk++e5SjJ86c9/mn3jvG6fYuu6KVGZZlJR721TT7Pc4j2C4Z+CLykojsGuBj6WAXUdVuVZ0N5AJzRWT6RbZdoapeVfVmZmYOdgljLukr8wuJjhL+34tnD1br7O7hV5vKuGrSWKZ7UhyqzoSzxTNziI2WkB+odsnAV9X5qjp9gI9ngOMikg3gu629xGs1Aq8BC/0v3ZihyUpO4MFrJ7FmRxW7Kk/95fl1O6upPtXGQ9fbkDQzPOmj4rixaBx/3l5F1xDO+Rhp/rZ01gAP+O4/ADxz7gYikikiqb77icB8YPhTrYzxw/IbJpGWFMv3X+j9J9h3vdrJmaO4ccqAHUljBuWuEg91ze1sOhS6oxb8DfxHgAUicgBY4HuMiOSIyDrfNtnAqyJSCrxPbw9/rZ/rGjMsyQmxfOmjhWw8UM/GA3W8c7iBXZVNPHjdJKKibIyCGb6bpo4jJTGWVSE8asGvg41V9QRw8wDPVwG3+u6XAsX+rGNMIH1q3gQef/MIjzy/j6zkBMaOiuPOYo/TZZkwFx8TzeKZ2Ty9tYLT7V0heS6HnWlrIk58TDRf/9gUdlc18cq+Wu67Kp+EWLterfHfshIPbZ09PL+z2ulSBmSBbyLS0lkeLs9OJi4mivvm2fVqTWCUTEgjf2wSq7eF5jH5ofczhzEjICpK+MUnS6hpamPs6HinyzEuIdI7auEnLx+gqrGVnNTQOonP9vBNxCrIGMW8SWOdLsO4zLLiXFThzyE4asEC3xhjAmjC2CS8+Wms3lo55KutBZsFvjHGBNiyklwO1J5mV2VojVqwwDfGmAC7bUY2cdFRrAqxUQsW+MYYE2ApSbHMnzaONdurhnR5zWCzwDfGmCC4sziXE2c62HggdKb+WuAbY0wQ3DAlk7SkWJ4OoTn5FvjGGBMEcTFRLJmVw4Y9xznV2ul0OYAFvjHGBM2dJbl0dIXOqAULfGOMCZJZuSlMyhzFqhAZtWCBb4wxQSIiLCv28N6RBsobWpwuxwLfGGOC6Q7f6O0/h8BevgW+McYEUW5aEh+ZmM7qbc6PWrDAN8aYILurJJfD9WfYXt7oaB0W+MYYE2SLZownPibK8Tn5FvjGGBNkYxJiueWK8azZUUVHl3OjFizwjTFmBCwr9tDY0slrH9Q6VoMFvjHGjIDrCjPIGB3HKgdHLfgV+CKSLiIbROSA7zbtIttGi8g2EVnrz5rGGBOOYqKjWDLLwyv7amls6XCkBn/38B8GXlbVQuBl3+ML+TKw18/1jDEmbC0r8dDR3cPaUmdGLfgb+EuBJ3z3nwDuGGgjEckFbgNW+rmeMcaErStykpmSNdqxo3X8DfwsVa0G8N2Ou8B2Pwa+CVzy19MislxENovI5rq60JkjbYwx/hIRlpXksuXoScrqz4z4+pcMfBF5SUR2DfCxdDALiMhioFZVtwxme1VdoapeVfVmZmYO5kuMMSZsLJ2dgwiO7OXHXGoDVZ1/oc+JyHERyVbVahHJBgY63ugaYImI3AokAMki8jtV/dSwqzbGmDCVnZLI1ZPHsnpbJV+ZX4iIjNja/rZ01gAP+O4/ADxz7gaq+m1VzVXVAuAe4BULe2NMJFtWnMuxhha2HD05ouv6G/iPAAtE5ACwwPcYEckRkXX+FmeMMW60cPp4EmOjR3xOvl+Br6onVPVmVS303Tb4nq9S1VsH2P41VV3sz5rGGBPuRsXHsHD6eNbuqKKts3vE1rUzbY0xxgF3Fntoauvi1X0jN2rBAt8YYxxwzWUZjBsTz9MjOGrBAt8YYxwQHSXcUezhtQ9qaTgzMqMWLPCNMcYhy0o8dPUoz+6oGpH1LPCNMcYhU8cnc3l28ogdrWOBb4wxDrqrxMOO8kYO1Z0O+loW+MYY46Als3KIElg9Ar+8tcA3xhgHjUtO4LrCTFZvq6SnR4O6lgW+McY4bFmJh8rGVt4rawjqOhb4xhjjsFumjWdUXHTQ2zoW+MYY47DEuGgWzchm3c7qoI5asMA3xpgQsKzYQ3N7Fxv2HA/aGhb4xhgTAuZNGktOSgKrtlYEbQ0LfGOMCQFRUcLSYg9vHKinrrk9OGsE5VWNMcYM2bJiD909ypogjVqwwDfGmBBRmDWGGZ4UVm8LTlvHAt8YY0LIffPymZWbSkdXT8Bf+5IXMTfGGDNy7r4yj7uvzAvKa9sevjHGRAgLfGOMiRAW+MYYEyH86uGLSDrwB6AAKAPuVtWTA2xXBjQD3UCXqnr9WdcYY8zQ+buH/zDwsqoWAi/7Hl/ITao628LeGGOc4W/gLwWe8N1/ArjDz9czxhgTJP4GfpaqVgP4bsddYDsFXhSRLSKy/GIvKCLLRWSziGyuq6vzszxjjDF9LtnDF5GXgPEDfOqfhrDONapaJSLjgA0isk9V3xhoQ1VdAawA8Hq9wb38izHGRBBRHX6misgHwI2qWi0i2cBrqlp0ia/5DnBaVX8wiNevA44Os7wMoH6YXxvq7L2FLze/P3tvoSFfVTMH+oS/Z9quAR4AHvHdPnPuBiIyCohS1Wbf/VuAfx3Mi1+o6MEQkc1u/QWxvbfw5eb3Z+8t9Pnbw38EWCAiB4AFvseISI6IrPNtkwW8KSI7gPeA51T1BT/XNcYYM0R+7eGr6gng5gGerwJu9d0/DMzyZx1jjDH+c/OZtiucLiCI7L2FLze/P3tvIc6vX9oaY4wJH27ewzfGGNOPBb4xxkQI1wW+iCwUkQ9E5KCIXGy2T9gRkTwReVVE9orIbhH5stM1BZqIRIvINhFZ63QtgSQiqSLyJxHZ5/v7u8rpmgJJRL7q+ze5S0R+LyIJTtc0XCLyuIjUisiufs+li8gGETngu01zssbhclXgi0g08DNgETANuFdEpjlbVUB1AV9T1cuBecAXXfb+AL4M7HW6iCD4CfCCqk6l96g117xHEfEAfw94VXU6EA3c42xVfvk1sPCc54YyKDJkuSrwgbnAQVU9rKodwFP0DnhzBVWtVtWtvvvN9IaGx9mqAkdEcoHbgJVO1xJIIpIMXA/8EkBVO1S10dGiAi8GSBSRGCAJqHK4nmHzjX1pOOdpVwyKdFvge4Dyfo8rcFEg9iciBUAx8K7DpQTSj4FvAoG/erOzJgF1wK987aqVvrPOXUFVK4EfAMeAauCUqr7obFUBN9hBkSHNbYEvAzznuuNORWQ08DTwFVVtcrqeQBCRxUCtqm5xupYgiAFKgF+oajFwhjBtCQzE189eCkwEcoBRIvIpZ6syA3Fb4FcA/S/3nksY/2g5EBGJpTfsn1TVVU7XE0DXAEt8V0d7CvioiPzO2ZICpgKoUNW+n8b+RO83ALeYDxxR1TpV7QRWAVc7XFOgHfcNiMR3W+twPcPitsB/HygUkYkiEkfvL47WOFxTwIiI0NsH3quqP3S6nkBS1W+raq6qFtD79/aKqrpiL1FVa4ByEembJHszsMfBkgLtGDBPRJJ8/0ZvxkW/lPbpGxQJFxgUGQ78nZYZUlS1S0S+BKyn90iBx1V1t8NlBdI1wH3AThHZ7nvuH1V13YW/xISIvwOe9O2IHAY+7XA9AaOq74rIn4Ct9B5Jto0wHkUgIr8HbgQyRKQC+Bd6B0P+UUQ+S+83uI87V+Hw2WgFY4yJEG5r6RhjjLkAC3xjjIkQFvjGGBMhLPCNMSZCWOAbY0yEsMA3xpgIYYFvjDER4v8DRP8DII9WqGIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Cell State 값 확인\n",
    "print(cx_tmp)\n",
    "plt.plot(cx_tmp)\n",
    "plt.show()\n",
    "#한번의 loop에 Cell State값이 변하고있음을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1d33ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, labels):\n",
    "    #다중분류를 위한 대표적인 손실함수\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    loss = loss_function(outputs, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7842ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_target[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "de091eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5495, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "current_loss = loss_fn(output, sample_target[:, -1])\n",
    "print(current_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a06237dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class MetricCalculator():\n",
    "    \"\"\"\n",
    "    loss와 accuracy를 기록하기 위한 도구입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.accuracy = 0\n",
    "        self.loss_accumulated = 0\n",
    "        self.average_loss = 0\n",
    "        self.updated_cnt = 0\n",
    "\n",
    "        self.predicted_labels_holder = []\n",
    "        self.actual_labels_holder = []\n",
    "\n",
    "    def update(self, outputs, labels, loss):\n",
    "        self.updated_cnt += 1\n",
    "\n",
    "        predicted_labels = outputs.max(1)[1]\n",
    "        self.predicted_labels_holder.append(predicted_labels)\n",
    "        self.actual_labels_holder.append(labels)\n",
    "        self.loss_accumulated += loss\n",
    "\n",
    "\n",
    "    def calculate_metric(self):\n",
    "\n",
    "        predicted_labels = torch.cat(self.predicted_labels_holder).cpu().numpy()\n",
    "        actual_labels = torch.cat(self.actual_labels_holder).cpu().numpy()\n",
    "\n",
    "        self.accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "        self.average_loss = self.loss_accumulated / self.updated_cnt\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.accuracy = 0\n",
    "        self.loss_accumulated = 0\n",
    "        self.average_loss = 0\n",
    "        self.updated_cnt = 0\n",
    "\n",
    "    def export(self):\n",
    "        return {\n",
    "            'loss': self.average_loss,\n",
    "            'accuracy': self.accuracy,\n",
    "        }\n",
    "    \n",
    "def save_checkpoint(state, is_best, checkpoint, epoch):\n",
    "    \"\"\"\n",
    "    모델을 저장합니다.\n",
    "    베스트 모델인 경우 카피본을 하나 더 만듭니다.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(checkpoint, 'e{:02d}.pth.tar'.format(epoch))\n",
    "    if not os.path.exists(checkpoint):\n",
    "        print(\"Checkpoint Directory does not exist! Making directory {}\".format(checkpoint))\n",
    "        os.makedirs(checkpoint)\n",
    "    else:\n",
    "        print(\"Checkpoint Directory exists!\")\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'best.pth.tar'))\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer=None):\n",
    "    \"\"\"\n",
    "    저장된 모델 파라미터를 불러와 업데이트합니다.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint):\n",
    "        raise (\"File doesn't exist {}\".format(checkpoint))\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optim_dict'])\n",
    "\n",
    "    return checkpoint\n",
    "  \n",
    "def save_dict_to_json(d, json_path):\n",
    "    \"\"\"\n",
    "    dictionary를 json 파일로 저장한다.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        # We need to convert the values to float for json (it doesn't accept np.array, np.float, )\n",
    "        d = {k: float(v) for k, v in d.items()}\n",
    "        json.dump(d, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2d46677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM 모델을 통해 이름, 성별,국가 데이터를 학습시킵니다.\n",
    "def train(model, optimizer, loss_fn, data_iterator, num_steps):  \n",
    "    metric_watcher = MetricCalculator()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    for ix, batch in enumerate(data_iterator):\n",
    "        inputs = batch.babyname[:, :-1].to(device)\n",
    "        targets = batch.babyname[:, 1:].to(device)\n",
    "        sex = batch.sex.float().to(device)\n",
    "        origin = batch.origin.float().to(device)\n",
    "\n",
    "        hidden = model.init_hidden(inputs.size(0))\n",
    "\n",
    "        loss = 0.0\n",
    "\n",
    "        for step in range(inputs.size(1)):\n",
    "            outputs, hidden = model.forward(origin, sex, inputs[:, step], hidden)\n",
    "            current_loss = loss_fn(outputs, targets[:, step])\n",
    "\n",
    "            metric_watcher.update(outputs, targets[:, step], current_loss)\n",
    "            loss += current_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    metric_watcher.calculate_metric()\n",
    "    metrics_string = \"loss: {:05.3f}, acc: {:05.3f}\".format(\n",
    "        metric_watcher.average_loss,\n",
    "        metric_watcher.accuracy\n",
    "    )\n",
    "    print(\"- Train metrics: \" + metrics_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1559fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss 값을 담기 위한 list\n",
    "loss_list=[ ]\n",
    "\n",
    "def evaluate(model, loss_fn, data_iterator):  \n",
    "    metric_watcher = MetricCalculator()\n",
    "\n",
    "    # 모델에 적합시킨다.\n",
    "    model.eval()\n",
    "\n",
    "    for ix, batch in enumerate(data_iterator):\n",
    "        inputs = batch.babyname[:, :-1].to(device)\n",
    "        targets = batch.babyname[:, 1:].to(device)\n",
    "        sex = batch.sex.float().to(device)\n",
    "        origin = batch.origin.float().to(device)\n",
    "\n",
    "        hidden = model.init_hidden(inputs.size(0))\n",
    "\n",
    "        loss = 0.0\n",
    "        \n",
    "        for step in range(inputs.size(1)):\n",
    "            outputs, hidden = model.forward(origin, sex, inputs[:, step], hidden)\n",
    "            current_loss = loss_fn(outputs, targets[:, step])\n",
    "\n",
    "            metric_watcher.update(outputs, targets[:, step], current_loss)\n",
    "            loss += current_loss\n",
    "    metric_watcher.calculate_metric()\n",
    "    \n",
    "    metric_watcher.calculate_metric()\n",
    "    metrics_string = \"loss: {:05.3f}, acc: {:05.3f}\".format(\n",
    "        metric_watcher.average_loss,\n",
    "        metric_watcher.accuracy\n",
    "    )\n",
    "    loss_list.append(metric_watcher.average_loss)\n",
    "    print(\"- Eval metrics: \" + metrics_string)\n",
    "    return metric_watcher.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c36a30de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_data_iter, val_data_iter, optimizer, loss_fn, model_dir, num_epochs, restore_file=None):\n",
    "  \n",
    "\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(args.model_dir, args.restore_file + '.pth.tar')\n",
    "        print(\"Restoring parameters from {}\".format(restore_path))\n",
    "        load_checkpoint(restore_path, model, optimizer)\n",
    "\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch\n",
    "        num_steps = len(train_data_iter.dataset.examples) // batch_size + 1\n",
    "        train(model, optimizer, loss_fn, train_data_iter, num_steps)\n",
    "\n",
    "        val_metrics = evaluate(model, loss_fn, train_data_iter)\n",
    "        val_acc = val_metrics['accuracy']\n",
    "        is_best = val_acc > best_val_acc\n",
    "\n",
    "        # Save weights\n",
    "        save_checkpoint({'epoch': epoch+1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optim_dict': optimizer.state_dict()},\n",
    "                              is_best=is_best,\n",
    "                              checkpoint=model_dir,\n",
    "                              epoch=epoch+1)\n",
    "\n",
    "        if is_best:\n",
    "            print(\"-- Found new best accuracy\")\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "            save_dict_to_json(val_metrics, best_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ba272e68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- start training for 3 epoch(s)\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torchtext\\legacy\\data\\field.py:353: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: loss: 1.611, acc: 0.503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torchtext\\legacy\\data\\field.py:353: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval metrics: loss: 1.500, acc: 0.528\n",
      "Checkpoint Directory exists!\n",
      "-- Found new best accuracy\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torchtext\\legacy\\data\\field.py:353: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: loss: 1.486, acc: 0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torchtext\\legacy\\data\\field.py:353: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval metrics: loss: 1.420, acc: 0.553\n",
      "Checkpoint Directory exists!\n",
      "-- Found new best accuracy\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torchtext\\legacy\\data\\field.py:353: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: loss: 1.440, acc: 0.550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torchtext\\legacy\\data\\field.py:353: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval metrics: loss: 1.389, acc: 0.559\n",
      "Checkpoint Directory exists!\n",
      "-- Found new best accuracy\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import shutil\n",
    "import json\n",
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "model_dir = \"saved\"\n",
    "#노트북 Ram 한계로 인하여 epoch는 3이 최대였습니다.\n",
    "num_epochs = 3\n",
    "\n",
    "train_iter = data_loader.train_iter\n",
    "val_iter = data_loader.val_iter\n",
    "\n",
    "# 모델\n",
    "model = Net(vocab_size=35, embedding_dim=100,\n",
    "                 nb_sex=2, nb_origin=52,\n",
    "                 lstm_nb_layers=1, lstm_hidden_dim=200,\n",
    "                fc_out=300, dropout_p=0.5, \n",
    "           ).to(device)\n",
    "\n",
    "# 최적화 알고리즘\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# 손실함수\n",
    "loss_fn = loss_fn\n",
    "\n",
    "# 학습 및 평가\n",
    "print(\"-- start training for {} epoch(s)\".format(num_epochs))\n",
    "train_and_evaluate(model, train_iter, val_iter, optimizer, loss_fn, model_dir, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6d2943b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20d014dc3d0>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "#print(loss_list)\n",
    "#stack 메소드를 통해 list안 각각의 토치원소들을 하나의 토치리스트로 변환\n",
    "loss=torch.stack(loss_list)\n",
    "#print(loss)\n",
    "loss_l=loss.tolist()\n",
    "#print(loss_l)\n",
    "#epoch에 따른 loss값을 그래프로 나타냄\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Losses')\n",
    "plt.plot(loss_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ecda31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"saved/best.pth.tar\"\n",
    "\n",
    "checkpoint = load_checkpoint(best_model_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b02ff9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(data_loader, net, prime, sex, origin):\n",
    "    \"\"\"\n",
    "    문화권, 성별과 시작 글자묶음을 입력받아\n",
    "    다음 글자들의 확률을 리턴한다.\n",
    "    \"\"\"\n",
    "    origin_tensor = data_loader.ORIGIN.process([origin]).float().to(device)\n",
    "    sex_tensor = data_loader.SEX.process([sex]).float().to(device)\n",
    "\n",
    "    prime = prime.lower()\n",
    "    prime_tensor = data_loader.BABYNAME.process([prime])[:, :-1].to(device)\n",
    "    bsz, prime_tensor_length = prime_tensor.size()\n",
    "\n",
    "    # 인풋을 모델에 넣어 출력합니다.\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        # batch_size = 1\n",
    "        hidden = net.init_hidden(1)\n",
    "\n",
    "        for step in range(prime_tensor_length):\n",
    "            with torch.no_grad():\n",
    "                outputs, hidden = net(origin_tensor, sex_tensor, prime_tensor[:, step], hidden)\n",
    "            probabilities = F.softmax(outputs, 1)\n",
    "\n",
    "    return probabilities.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "07d49747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "Beam-Search를 통해 후보 확률 계산\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "beam-search 부분은 난이도가 어려워 이해를 제대로 하지못해 발표영상에서 삭제하였습니다.\n",
    "\"\"\"\n",
    "def clean_beam_basket(basket, beam_width):\n",
    "    _tmp_basket = basket.copy()\n",
    "    to_remove = sorted(_tmp_basket.items(), key=lambda x: x[1], reverse=True)[beam_width:]\n",
    "    for item in to_remove:\n",
    "        _tmp_basket.pop(item[0])\n",
    "\n",
    "    return _tmp_basket\n",
    "\n",
    "def beam_search(data_loader, net, beam_width, alpha, prime, sex, origin):\n",
    "    # print(\"Sampling a {} {} name beginning with {}..\".format(origin, sex, prime))\n",
    "\n",
    "    beam_basket = OrderedDict()\n",
    "    beam_basket[prime] = 0.0\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    while True:\n",
    "        counter += 1\n",
    "\n",
    "        # 바스켓을 청소합니다.\n",
    "        beam_basket = clean_beam_basket(beam_basket, beam_width)\n",
    "\n",
    "        # 만약 바스켓에 모든 아이템이 <eos>가 있으면 루프를 멈춘다.\n",
    "        eos_cnt = 0\n",
    "        for k in beam_basket.keys():\n",
    "            if \"<eos>\" in k:\n",
    "                eos_cnt += 1\n",
    "        if eos_cnt == beam_width:\n",
    "            # print(\"all items have <eos>\")\n",
    "            break\n",
    "        new_entries = {}\n",
    "        to_remove = []\n",
    "        for k in beam_basket.keys():\n",
    "            if \"<eos>\" not in k:\n",
    "                probabilities = sample(data_loader, net, prime=k, sex=sex, origin=origin)\n",
    "                for ix, prob in enumerate(probabilities):\n",
    "                    new_k = k + data_loader.BABYNAME.vocab.itos[ix]\n",
    "                    added_probability = beam_basket[k] + torch.log(prob).item()\n",
    "                    len_k = len(k.replace(\"<eos>\", \"\"))\n",
    "                    normalized_probability = (1 / (len(k) ** alpha)) * added_probability\n",
    "                    new_entries[new_k] = normalized_probability\n",
    "                to_remove.append(k)\n",
    "    \n",
    "        # 그리고 기존 key를 beam_basket에서 지운다.\n",
    "        for k in to_remove:\n",
    "            beam_basket.pop(k)\n",
    "\n",
    "        # 새로운 키를 바스켓에 채워넣는다.\n",
    "        for k, v in new_entries.items():\n",
    "            beam_basket[k] = v\n",
    "\n",
    "    final_list = []\n",
    "    for k, v in beam_basket.items():\n",
    "        refined_k = k.replace(\"<eos>\", \"\").capitalize()\n",
    "        final_list.append(refined_k)\n",
    "        final_prob = np.exp(v)\n",
    "\n",
    "    return final_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "124b6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#최종 이름 출력함수\n",
    "def final_names(data_loader, model_dir, prime, sex, origin, beam_width=5, alpha=0.7):\n",
    "    model = Net(vocab_size=35, embedding_dim=100,\n",
    "                 nb_sex=2, nb_origin=52,\n",
    "                 lstm_nb_layers=1, lstm_hidden_dim=200,\n",
    "                fc_out=300, dropout_p=0.5, \n",
    "           ).to(device)\n",
    "    \n",
    "    weight_path = os.path.join(model_dir, \"best.pth.tar\")\n",
    "    checkpoint = load_checkpoint(weight_path, model)\n",
    "\n",
    "    final_list = beam_search(data_loader, model, prime=prime, sex=sex, origin=origin, beam_width=beam_width, alpha=alpha)\n",
    "   \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "40169fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_names(data_loader, \"saved\", \"Ma\", \"girl\", \"British\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f28b9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_names(data_loader, \"saved\", \"Ma\", \"girl\", \"Japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "31e310b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torchtext\\legacy\\data\\field.py:353: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hyelon', 'Hyerin', 'Hyeron', 'Hyerino', 'Hyemino']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_names(data_loader, \"saved\", \"hye\", \"boy\", \"Spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "47ba52b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hyean', 'Hyein', 'Hyeon', 'Hyeun', 'Hyeeun']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#스페인과 비교하여 한국의 작명 스타일이 드러남.\n",
    "final_names(data_loader, \"saved\", \"hye\", \"girl\", \"Korea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9115c793",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yumi', 'Yuko', 'Yushi', 'Yumika', 'Yumiko']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_names(data_loader, \"saved\", \"yu\", \"girl\", \"Japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c6f16cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yun', 'Yunne', 'Yunhi', 'Yumin', 'Yunney']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_names(data_loader, \"saved\", \"yu\", \"girl\", \"Korea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "df379733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yulan', 'Yusty', 'Yuland', 'Yustin', 'Yushan']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_names(data_loader, \"saved\", \"yu\", \"boy\", \"American\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
